{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Price Movement Predictor\n",
    "\n",
    "**Date:** 18.03.25\n",
    "\n",
    "This notebook implements a comprehensive semi-supervised learning framework for DAO price movement prediction using LSTM models with Monte Carlo attention mechanisms.\n",
    "\n",
    "## Key Features\n",
    "- **Monte Carlo Attention**: Advanced attention mechanism with uncertainty quantification\n",
    "- **Semi-Supervised Learning**: Iterative pseudo-labeling with confidence thresholds\n",
    "- **Multi-Model Architecture**: LSTM + XGBoost ensemble\n",
    "- **Comprehensive Evaluation**: Performance tracking across iterations\n",
    "\n",
    "## Methodology\n",
    "1. **Baseline Training**: Train LSTM classifier on labeled data\n",
    "2. **Pseudo-Labeling**: Generate high-confidence labels for unlabeled data\n",
    "3. **Iterative Refinement**: Progressively improve model with augmented data\n",
    "4. **Ensemble Prediction**: Combine LSTM features with XGBoost for final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import custom modules\n",
    "from config import *\n",
    "from utils.data_preprocessing import create_sequences, prepare_preprocessor\n",
    "from utils.pseudo_labeling import (batch_pseudo_labeling_monte_carlo, \n",
    "                                  batch_pseudo_labeling_regular,\n",
    "                                  split_labeled_unlabeled, update_training_data)\n",
    "\n",
    "from utils.evaluation import (compute_classification_metrics, \n",
    "                             plot_metrics_over_iterations, \n",
    "                             save_results, log_memory_usage)\n",
    "\n",
    "from models.monte_carlo_attention import SingleInputLSTMClassifier as MCLSTMClassifier\n",
    "from models.lstm_classifier import SingleInputLSTMClassifier as RegularLSTMClassifier\n",
    "\n",
    "print(\"✓ All modules imported successfully\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✓ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    preprocessed_data = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, \"processed_dao_data.csv\"))\n",
    "    print(f\"✓ Data loaded successfully: {preprocessed_data.shape}\")\n",
    "    \n",
    "    # Optional: Sample data for faster experimentation\n",
    "    if len(preprocessed_data['Slug_Santiment'].unique()) > 40:\n",
    "        selected_groups = np.random.choice(\n",
    "            preprocessed_data['Slug_Santiment'].unique(), \n",
    "            size=40, replace=False\n",
    "        )\n",
    "        preprocessed_data = preprocessed_data[\n",
    "            preprocessed_data['Slug_Santiment'].isin(selected_groups)\n",
    "        ]\n",
    "        print(f\"✓ Sampled to 40 groups: {preprocessed_data.shape}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Processed data not found. Please run data preprocessing first.\")\n",
    "    raise\n",
    "\n",
    "log_memory_usage(\"After data loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare features and preprocessing pipeline\n",
    "print(\"Setting up feature preprocessing...\")\n",
    "\n",
    "# Define feature columns from config\n",
    "feature_columns_return = [\n",
    "    'Slug_Santiment_Encoded', 'vp_gini', 'transaction_volume',\n",
    "    'velocity', 'unique_sv_total_1h', 'dev_activity', 'price_usd',\n",
    "    'price_btc_usd', 'dao_age', 'marketSegment_Encoded', 'Total TVL in USD', \n",
    "    'SP500', 'MA_14', 'EMA_14', 'RSI_14', \n",
    "    '14_social_media_activity', '30_social_media_activity', \n",
    "    '14_day_social_ewma', '30_day_social_ewma', '60_day_social_ewma', '90_day_social_ewma'\n",
    "]\n",
    "\n",
    "# Add lagged features\n",
    "lagged_feature_columns = [f'{feature}_lag_{i}' for feature in LAG_FEATURES for i in range(1, LAG_PERIODS + 1)]\n",
    "feature_columns_return.extend(lagged_feature_columns)\n",
    "\n",
    "# Prepare preprocessor\n",
    "preprocessor = prepare_preprocessor(feature_columns_return, LOG_TRANSFORM_COLUMNS)\n",
    "\n",
    "print(f\"✓ Feature preprocessing configured\")\n",
    "print(f\"  - Total features: {len(feature_columns_return)}\")\n",
    "print(f\"  - Log transform features: {len(LOG_TRANSFORM_COLUMNS)}\")\n",
    "print(f\"  - Lagged features: {len(lagged_feature_columns)}\")\n",
    "\n",
    "log_memory_usage(\"After feature setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and sequence creation\n",
    "print(\"Processing data for LSTM training...\")\n",
    "\n",
    "# Sort and clean data\n",
    "preprocessed_data.sort_values(by=['Slug_Santiment_Encoded', 'vote_date'], inplace=True)\n",
    "train_data = preprocessed_data.copy()\n",
    "\n",
    "# Prepare features and targets\n",
    "X_train = train_data[feature_columns_return].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "y_train = train_data['price_trend'].values\n",
    "y_train = np.where(np.isnan(y_train) | np.isinf(y_train), 0, y_train)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Create feature names for scaled data\n",
    "log_transformed_features = [f\"log_{col}\" for col in LOG_TRANSFORM_COLUMNS]\n",
    "scaled_features = [col for col in feature_columns_return \n",
    "                  if col not in LOG_TRANSFORM_COLUMNS + ['Slug_Santiment_Encoded', 'marketSegment_Encoded', 'dao_age']]\n",
    "passthrough_features = ['Slug_Santiment_Encoded', 'marketSegment_Encoded', 'dao_age']\n",
    "columns = log_transformed_features + scaled_features + passthrough_features\n",
    "\n",
    "# Create DataFrame with processed features\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=columns, index=train_data.index)\n",
    "X_train_scaled_df['vote_date'] = train_data['vote_date']\n",
    "X_train_scaled_df['price_trend'] = y_train\n",
    "\n",
    "# Group by organization for sequence creation\n",
    "grouped_train = X_train_scaled_df.groupby(train_data['Slug_Santiment'])\n",
    "\n",
    "print(\"✓ Data preprocessing completed\")\n",
    "print(f\"  - Training samples: {len(X_train_scaled_df)}\")\n",
    "print(f\"  - Organizations: {len(grouped_train)}\")\n",
    "\n",
    "log_memory_usage(\"After data preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences and train/test split\n",
    "print(\"Creating sequences and splitting data...\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data['vote_date'] = pd.to_datetime(train_data['vote_date'], errors='coerce')\n",
    "max_date = train_data['vote_date'].max()\n",
    "test_start_date = max_date - pd.Timedelta(days=30)\n",
    "\n",
    "train_input_streams = {}\n",
    "train_target_streams = {}\n",
    "test_input_streams = {}\n",
    "test_target_streams = {}\n",
    "\n",
    "for name, group in grouped_train:\n",
    "    X_seq, y_seq, dates = create_sequences(group, WINDOW_SIZE)\n",
    "    dates = np.array(dates, dtype='datetime64')\n",
    "    train_mask = dates < test_start_date\n",
    "    test_mask = dates >= test_start_date\n",
    "    \n",
    "    train_input_streams[name] = X_seq[train_mask]\n",
    "    train_target_streams[name] = y_seq[train_mask]\n",
    "    test_input_streams[name] = X_seq[test_mask]\n",
    "    test_target_streams[name] = y_seq[test_mask]\n",
    "\n",
    "num_features = train_input_streams[list(train_input_streams.keys())[0]].shape[2]\n",
    "print(f\"✓ Sequences created\")\n",
    "print(f\"  - Feature dimensions: {num_features}\")\n",
    "print(f\"  - Train organizations: {len(train_input_streams)}\")\n",
    "print(f\"  - Test organizations: {len(test_input_streams)}\")\n",
    "\n",
    "log_memory_usage(\"After sequence creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "Configure different experimental settings for comparison:\n",
    "- **Baseline Model**: Regular LSTM without Monte Carlo attention\n",
    "- **Monte Carlo Model**: LSTM with Monte Carlo attention mechanism\n",
    "- **Semi-Supervised Variants**: Different labeled data percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "EXPERIMENTS = [\n",
    "    {\n",
    "        'name': 'baseline_100',\n",
    "        'labeled_percentage': 1.0,\n",
    "        'model_type': 'regular',\n",
    "        'use_monte_carlo': False,\n",
    "        'description': 'Baseline with 100% labeled data'\n",
    "    },\n",
    "    {\n",
    "        'name': 'semi_supervised_90_regular',\n",
    "        'labeled_percentage': 0.9,\n",
    "        'model_type': 'regular',\n",
    "        'use_monte_carlo': False,\n",
    "        'description': 'Semi-supervised with 90% labeled data (regular pseudo-labeling)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'semi_supervised_90_mc',\n",
    "        'labeled_percentage': 0.9,\n",
    "        'model_type': 'monte_carlo',\n",
    "        'use_monte_carlo': True,\n",
    "        'description': 'Semi-supervised with 90% labeled data (Monte Carlo pseudo-labeling)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'semi_supervised_80_regular',\n",
    "        'labeled_percentage': 0.8,\n",
    "        'model_type': 'regular',\n",
    "        'use_monte_carlo': False,\n",
    "        'description': 'Semi-supervised with 80% labeled data (regular pseudo-labeling)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'semi_supervised_80_mc',\n",
    "        'labeled_percentage': 0.8,\n",
    "        'model_type': 'monte_carlo',\n",
    "        'use_monte_carlo': True,\n",
    "        'description': 'Semi-supervised with 80% labeled data (Monte Carlo pseudo-labeling)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'semi_supervised_60_regular',\n",
    "        'labeled_percentage': 0.6,\n",
    "        'model_type': 'regular',\n",
    "        'use_monte_carlo': False,\n",
    "        'description': 'Semi-supervised with 60% labeled data (regular pseudo-labeling)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'semi_supervised_60_mc',\n",
    "        'labeled_percentage': 0.6,\n",
    "        'model_type': 'monte_carlo',\n",
    "        'use_monte_carlo': True,\n",
    "        'description': 'Semi-supervised with 60% labeled data (Monte Carlo pseudo-labeling)'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "for exp in EXPERIMENTS:\n",
    "    print(f\"  - {exp['name']}: {exp['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi-supervised training function\n",
    "def run_semi_supervised_experiment(experiment_config, train_input_streams, train_target_streams):\n",
    "    \"\"\"Run a single semi-supervised learning experiment.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running Experiment: {experiment_config['name']}\")\n",
    "    print(f\"Description: {experiment_config['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    if experiment_config['model_type'] == 'monte_carlo':\n",
    "        model = MCLSTMClassifier(input_dim=num_features).to(device)\n",
    "    else:\n",
    "        model = RegularLSTMClassifier(input_dim=num_features).to(device)\n",
    "    \n",
    "    # Initialize metrics tracking\n",
    "    iteration_metrics = []\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Run iterative training\n",
    "    for iteration in range(NUM_ITERATIONS):\n",
    "        print(f\"\\n=== Iteration {iteration + 1}/{NUM_ITERATIONS} ===\")\n",
    "        \n",
    "        all_y_true, all_y_pred, all_y_prob = [], [], []\n",
    "        processed_orgs = set()\n",
    "        \n",
    "        for name in train_input_streams.keys():\n",
    "            X = train_input_streams[name]\n",
    "            y = train_target_streams[name]\n",
    "            \n",
    "            if len(X) < WINDOW_SIZE + 1:\n",
    "                continue\n",
    "            \n",
    "            # Split into labeled and unlabeled\n",
    "            X_labeled, y_labeled, X_unlabeled, _ = split_labeled_unlabeled(\n",
    "                X, y, experiment_config['labeled_percentage']\n",
    "            )\n",
    "            \n",
    "            # Time series cross-validation\n",
    "            tscv = TimeSeriesSplit(n_splits=3)\n",
    "            for split_id, (train_idx, val_idx) in enumerate(tscv.split(X_labeled)):\n",
    "                print(f\"Training {name}, split {split_id + 1}\")\n",
    "                \n",
    "                X_train_fold, X_val_fold = X_labeled[train_idx], X_labeled[val_idx]\n",
    "                y_train_fold, y_val_fold = y_labeled[train_idx], y_labeled[val_idx]\n",
    "                \n",
    "                # Train model\n",
    "                train_dataset = torch.utils.data.TensorDataset(\n",
    "                    torch.tensor(X_train_fold, dtype=torch.float32),\n",
    "                    torch.tensor(y_train_fold, dtype=torch.float32)\n",
    "                )\n",
    "                train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    "                )\n",
    "                \n",
    "                optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "                scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, 'min', patience=PATIENCE, factor=0.1, verbose=True\n",
    "                )\n",
    "                \n",
    "                # Training loop\n",
    "                model.train()\n",
    "                for epoch in range(NUM_EPOCHS):\n",
    "                    epoch_loss = 0.0\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        X_batch = X_batch.to(device)\n",
    "                        y_batch = y_batch.to(device).unsqueeze(1)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(X_batch)\n",
    "                        loss = criterion(output, y_batch)\n",
    "                        loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                        epoch_loss += loss.item()\n",
    "                    \n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        X_val_tensor = torch.tensor(X_val_fold, dtype=torch.float32).to(device)\n",
    "                        val_output = model(X_val_tensor)\n",
    "                        val_loss = criterion(val_output, torch.tensor(y_val_fold, dtype=torch.float32).to(device).unsqueeze(1))\n",
    "                    \n",
    "                    scheduler.step(val_loss)\n",
    "                    \n",
    "                    if epoch % 10 == 0:\n",
    "                        print(f\"Epoch {epoch}, Train Loss: {epoch_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "                \n",
    "                # XGBoost ensemble\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    train_features = model(torch.tensor(X_train_fold, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "                    val_features = model(torch.tensor(X_val_fold, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "                \n",
    "                gbm = xgb.XGBClassifier(**XGBOOST_PARAMS)\n",
    "                gbm.fit(train_features, y_train_fold)\n",
    "                val_prob = gbm.predict_proba(val_features)[:, 1]\n",
    "                val_pred = (val_prob > 0.5).astype(int)\n",
    "                \n",
    "                all_y_true.extend(y_val_fold)\n",
    "                all_y_pred.extend(val_pred)\n",
    "                all_y_prob.extend(val_prob)\n",
    "                processed_orgs.add(name)\n",
    "            \n",
    "            # Pseudo-labeling for next iteration\n",
    "            if len(X_unlabeled) > 0 and experiment_config['labeled_percentage'] < 1.0:\n",
    "                if experiment_config['use_monte_carlo']:\n",
    "                    pseudo_labels = batch_pseudo_labeling_monte_carlo(\n",
    "                        model, X_unlabeled, BATCH_SIZE, CONFIDENCE_THRESHOLD, \n",
    "                        VARIANCE_THRESHOLD, NUM_MC_SAMPLES, device\n",
    "                    )\n",
    "                else:\n",
    "                    pseudo_labels = batch_pseudo_labeling_regular(\n",
    "                        model, X_unlabeled, BATCH_SIZE, CONFIDENCE_THRESHOLD, device\n",
    "                    )\n",
    "                \n",
    "                # Update training data\n",
    "                train_input_streams[name], train_target_streams[name] = update_training_data(\n",
    "                    X_labeled, y_labeled, X_unlabeled, pseudo_labels\n",
    "                )\n",
    "        \n",
    "        # Compute metrics for this iteration\n",
    "        if all_y_true:\n",
    "            metrics = compute_classification_metrics(\n",
    "                np.array(all_y_true), np.array(all_y_pred), np.array(all_y_prob)\n",
    "            )\n",
    "            metrics['iteration'] = iteration + 1\n",
    "            iteration_metrics.append(metrics)\n",
    "            \n",
    "            print(f\"\\nIteration {iteration + 1} Results:\")\n",
    "            print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "            print(f\"ROC AUC: {metrics['ROC_AUC']:.4f}\")\n",
    "            print(f\"PR AUC: {metrics['PR_AUC']:.4f}\")\n",
    "        \n",
    "        log_memory_usage(f\"After iteration {iteration + 1}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(iteration_metrics)\n",
    "    results_path = os.path.join(RESULTS_PATH, f\"{experiment_config['name']}_results.csv\")\n",
    "    save_results(results_df, results_path)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(MODEL_PATH, f\"{experiment_config['name']}_model.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    return results_df, model\n",
    "\n",
    "print(\"✓ Semi-supervised training function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run all experiments\n",
    "all_results = {}\n",
    "all_models = {}\n",
    "\n",
    "for experiment in EXPERIMENTS:\n",
    "    try:\n",
    "        results_df, model = run_semi_supervised_experiment(\n",
    "            experiment, train_input_streams.copy(), train_target_streams.copy()\n",
    "        )\n",
    "        all_results[experiment['name']] = results_df\n",
    "        all_models[experiment['name']] = model\n",
    "        \n",
    "        print(f\"✓ Completed experiment: {experiment['name']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in experiment {experiment['name']}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative analysis and visualization\n",
    "print(\"Generating comparative analysis...\")\n",
    "\n",
    "try:\n",
    "    # Combine all results for comparison\n",
    "    comparison_data = []\n",
    "    \n",
    "    for exp_name, results_df in all_results.items():\n",
    "        if not results_df.empty:\n",
    "            # Get final iteration metrics\n",
    "            final_metrics = results_df.iloc[-1].to_dict()\n",
    "            final_metrics['Experiment'] = exp_name\n",
    "            comparison_data.append(final_metrics)\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Save comparison results\n",
    "        comparison_path = os.path.join(RESULTS_PATH, 'experiment_comparison.csv')\n",
    "        save_results(comparison_df, comparison_path)\n",
    "        \n",
    "        # Create comparison plots\n",
    "        metrics_to_plot = ['Accuracy', 'ROC_AUC', 'PR_AUC', 'F1_Score']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            if metric in comparison_df.columns:\n",
    "                bars = axes[i].bar(comparison_df['Experiment'], comparison_df[metric])\n",
    "                axes[i].set_title(f'{metric} Comparison')\n",
    "                axes[i].set_ylabel(metric)\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar in bars:\n",
    "                    height = bar.get_height()\n",
    "                    axes[i].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                               f'{height:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(RESULTS_PATH, 'experiment_comparison.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot iteration evolution for each experiment\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        for i, (exp_name, results_df) in enumerate(all_results.items()):\n",
    "            if not results_df.empty and 'iteration' in results_df.columns:\n",
    "                plt.subplot(2, 3, i+1)\n",
    "                plt.plot(results_df['iteration'], results_df['Accuracy'], 'o-', label='Accuracy')\n",
    "                plt.plot(results_df['iteration'], results_df['ROC_AUC'], 's-', label='ROC AUC')\n",
    "                plt.xlabel('Iteration')\n",
    "                plt.ylabel('Metric Value')\n",
    "                plt.title(f'{exp_name}')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(RESULTS_PATH, 'iteration_evolution.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✓ Comparative analysis completed\")\n",
    "        print(f\"  - Results saved to: {comparison_path}\")\n",
    "        print(f\"  - Plots saved to: {RESULTS_PATH}\")\n",
    "        \n",
    "        # Print summary table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FINAL RESULTS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        summary_cols = ['Experiment', 'Accuracy', 'ROC_AUC', 'PR_AUC', 'F1_Score']\n",
    "        print(comparison_df[summary_cols].to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No successful experiments to compare\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in comparative analysis: {e}\")\n",
    "\n",
    "log_memory_usage(\"Final memory usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "try:\n",
    "    test_results = {}\n",
    "    \n",
    "    for exp_name, model in all_models.items():\n",
    "        print(f\"\\nEvaluating {exp_name} on test set...\")\n",
    "        \n",
    "        all_test_true, all_test_pred, all_test_prob = [], [], []\n",
    "        \n",
    "        for name in test_input_streams.keys():\n",
    "            X_test = test_input_streams[name]\n",
    "            y_test = test_target_streams[name]\n",
    "            \n",
    "            if len(X_test) > 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "                    test_features = model(X_test_tensor).cpu().numpy()\n",
    "                \n",
    "                # Use a simple threshold for final prediction (could use saved XGBoost)\n",
    "                test_pred = (test_features > 0.5).astype(int).flatten()\n",
    "                test_prob = test_features.flatten()\n",
    "                \n",
    "                all_test_true.extend(y_test)\n",
    "                all_test_pred.extend(test_pred)\n",
    "                all_test_prob.extend(test_prob)\n",
    "        \n",
    "        if all_test_true:\n",
    "            test_metrics = compute_classification_metrics(\n",
    "                np.array(all_test_true), np.array(all_test_pred), np.array(all_test_prob)\n",
    "            )\n",
    "            test_results[exp_name] = test_metrics\n",
    "            \n",
    "            print(f\"Test Accuracy: {test_metrics['Accuracy']:.4f}\")\n",
    "            print(f\"Test ROC AUC: {test_metrics['ROC_AUC']:.4f}\")\n",
    "    \n",
    "    # Save test results\n",
    "    if test_results:\n",
    "        test_df = pd.DataFrame(test_results).T\n",
    "        test_df.to_csv(os.path.join(RESULTS_PATH, 'test_results.csv'))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST SET RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(test_df[['Accuracy', 'ROC_AUC', 'PR_AUC', 'F1_Score']].to_string(float_format='%.4f'))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in test evaluation: {e}\")\n",
    "\n",
    "print(\"\\nSemi-supervised learning analysis completed!\")\n",
    "print(f\"All results saved to: {RESULTS_PATH}\")\n",
    "print(f\"All models saved to: {MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
