{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b6b7ab",
   "metadata": {},
   "source": [
    "# DAO Price Movement Prediction - Data Preprocessing\n",
    "\n",
    "This notebook implements a comprehensive data preprocessing pipeline for predicting cryptocurrency price movements based on DAO governance activity, social sentiment, and technical indicators.\n",
    "\n",
    "## Objectives\n",
    "- Clean and prepare raw data for machine learning\n",
    "- Engineer relevant features for price movement prediction\n",
    "- Create target variables for classification tasks\n",
    "- Implement robust data validation and quality checks\n",
    "\n",
    "## Key Features\n",
    "- **Technical Indicators**: Moving averages, RSI, EMA\n",
    "- **Governance Features**: Voting activity, proposal metrics\n",
    "- **Social Features**: Social media sentiment and activity\n",
    "- **Market Features**: Price data, volume, market cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fd694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import custom modules\n",
    "from config import *\n",
    "from utils.data_preprocessing import *\n",
    "from utils.evaluation import log_memory_usage\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"✓ Available feature columns: {len(FEATURE_COLUMNS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "print(\"Loading raw data...\")\n",
    "file_path = os.path.join(DATA_PATH, \"regression_df_v24cc_v6.csv\")\n",
    "\n",
    "try:\n",
    "    raw_data = pd.read_csv(file_path)\n",
    "    print(f\"✓ Data loaded successfully\")\n",
    "    print(f\"  - Shape: {raw_data.shape}\")\n",
    "    print(f\"  - Date range: {raw_data['vote_date'].min()} to {raw_data['vote_date'].max()}\")\n",
    "    print(f\"  - Unique DAOs: {raw_data['Slug_Santiment'].nunique()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ File not found: {file_path}\")\n",
    "    raise\n",
    "\n",
    "# Log initial memory usage\n",
    "log_memory_usage(\"After data loading\")\n",
    "\n",
    "# Display basic info\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c87afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print(\"=== Data Quality Assessment ===\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_summary = raw_data.isnull().sum()\n",
    "missing_pct = (missing_summary / len(raw_data)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_summary,\n",
    "    'Missing_Percentage': missing_pct\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\nMissing values summary:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0].head(10))\n",
    "\n",
    "# Check for infinite values\n",
    "inf_cols = []\n",
    "for col in raw_data.select_dtypes(include=[np.number]).columns:\n",
    "    if np.isinf(raw_data[col]).any():\n",
    "        inf_cols.append(col)\n",
    "\n",
    "print(f\"\\nColumns with infinite values: {inf_cols}\")\n",
    "\n",
    "# Validate required columns exist\n",
    "required_cols = ['Slug_Santiment', 'vote_date', 'marketcap_usd_cleaned', 'price_usd']\n",
    "missing_required = [col for col in required_cols if col not in raw_data.columns]\n",
    "if missing_required:\n",
    "    print(f\"❌ Missing required columns: {missing_required}\")\n",
    "    raise ValueError(\"Required columns are missing from the dataset\")\n",
    "else:\n",
    "    print(\"✓ All required columns present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Encode categorical features\n",
    "print(\"Step 1: Encoding categorical features...\")\n",
    "\n",
    "try:\n",
    "    processed_data, encoders = encode_categorical_features(raw_data)\n",
    "    \n",
    "    print(f\"✓ Categorical encoding completed\")\n",
    "    print(f\"  - Slug_Santiment: {len(encoders['Slug_Santiment'].classes_)} unique values\")\n",
    "    print(f\"  - Market segments: {len(encoders['marketSegment'].classes_)} unique values\")\n",
    "    \n",
    "    # Store encoders for later use\n",
    "    os.makedirs(os.path.join(MODEL_PATH, 'encoders'), exist_ok=True)\n",
    "    import joblib\n",
    "    joblib.dump(encoders, os.path.join(MODEL_PATH, 'encoders', 'label_encoders.pkl'))\n",
    "    print(\"✓ Encoders saved for future use\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in categorical encoding: {e}\")\n",
    "    raise\n",
    "\n",
    "log_memory_usage(\"After categorical encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b835464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale large values and handle data types\n",
    "print(\"Step 2: Scaling large numerical values...\")\n",
    "\n",
    "# Scale market cap and transaction volume to billions using config\n",
    "try:\n",
    "    processed_data['marketcap_usd_cleaned'] = processed_data['marketcap_usd_cleaned'] / 1_000_000_000\n",
    "    processed_data['transaction_volume'] = processed_data['transaction_volume'] / 1_000_000_000\n",
    "    \n",
    "    print(\"✓ Values scaled to appropriate ranges (billions)\")\n",
    "    print(f\"  - Market cap range: {processed_data['marketcap_usd_cleaned'].min():.2f} - {processed_data['marketcap_usd_cleaned'].max():.2f}B\")\n",
    "    print(f\"  - Transaction volume range: {processed_data['transaction_volume'].min():.2f} - {processed_data['transaction_volume'].max():.2f}B\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in scaling: {e}\")\n",
    "    raise\n",
    "\n",
    "log_memory_usage(\"After scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate returns and create target variables\n",
    "print(\"Step 3: Calculating returns and define the target variable...\")\n",
    "try:\n",
    "    processed_data = calculate_returns_and_targets(processed_data)\n",
    "    \n",
    "    print(\"✓ Returns calculated and target variable created\")\n",
    "    \n",
    "    # Analyze target distribution\n",
    "    trend_counts = processed_data['price_trend'].value_counts()\n",
    "    total_valid = trend_counts.sum()\n",
    "    \n",
    "    print(f\"  - Price trend distribution:\")\n",
    "    print(f\"    Fall (0): {trend_counts.get(0, 0):,} ({trend_counts.get(0, 0)/total_valid:.1%})\")\n",
    "    print(f\"    Rise (1): {trend_counts.get(1, 0):,} ({trend_counts.get(1, 0)/total_valid:.1%})\")\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    if abs(trend_counts.get(0, 0)/total_valid - 0.5) > 0.2:\n",
    "        print(\"⚠️  Warning: Significant class imbalance detected. Consider adding more training data or taking a smaller sample size\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in return calculation: {e}\")\n",
    "    raise\n",
    "\n",
    "log_memory_usage(\"After return calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate technical indicators\n",
    "print(\"Step 4: Calculating technical indicators...\")\n",
    "try:\n",
    "    processed_data = calculate_technical_indicators(processed_data, TECHNICAL_INDICATORS)\n",
    "    \n",
    "    print(\"✓ Technical indicators calculated\")\n",
    "    print(f\"  - MA_{TECHNICAL_INDICATORS['MA_WINDOW']}: Moving Average ({TECHNICAL_INDICATORS['MA_WINDOW']} days)\")\n",
    "    print(f\"  - EMA_{TECHNICAL_INDICATORS['EMA_WINDOW']}: Exponential Moving Average ({TECHNICAL_INDICATORS['EMA_WINDOW']} days)\")\n",
    "    print(f\"  - RSI_{TECHNICAL_INDICATORS['RSI_WINDOW']}: Relative Strength Index ({TECHNICAL_INDICATORS['RSI_WINDOW']} days)\")\n",
    "    \n",
    "    # Validate indicator ranges\n",
    "    rsi_values = processed_data['RSI_14'].dropna()\n",
    "    if len(rsi_values) > 0:\n",
    "        print(f\"  - RSI range: {rsi_values.min():.1f} - {rsi_values.max():.1f}\")\n",
    "        if rsi_values.min() < 0 or rsi_values.max() > 100:\n",
    "            print(\"⚠️  Warning: RSI values outside expected range [0, 100]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in technical indicators: {e}\")\n",
    "    raise\n",
    "\n",
    "log_memory_usage(\"After technical indicators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d08e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Calculate activity features\n",
    "print(\"Step 5: Calculating governance and social activity features...\")\n",
    "try:\n",
    "    processed_data = calculate_activity_features(processed_data, GOVERNANCE_WINDOWS)\n",
    "    \n",
    "    print(\"✓ Activity features calculated\")\n",
    "    print(f\"  - Governance activity windows: {GOVERNANCE_WINDOWS}\")\n",
    "    print(f\"  - Social media activity windows: {SOCIAL_WINDOWS}\")\n",
    "    print(f\"  - Network activity windows: {GOVERNANCE_WINDOWS}\")\n",
    "    \n",
    "    # Validate some key features\n",
    "    key_features = ['14_day_gov_activity', '30_social_media_activity', '14_day_network_ewma']\n",
    "    for feature in key_features:\n",
    "        if feature in processed_data.columns:\n",
    "            non_null_count = processed_data[feature].notna().sum()\n",
    "            print(f\"  - {feature}: {non_null_count:,} non-null values\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in activity features: {e}\")\n",
    "    raise\n",
    "\n",
    "log_memory_usage(\"After activity features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c23c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create lagged features\n",
    "print(\"Step 6: Creating lagged features...\")\n",
    "try:\n",
    "    processed_data = create_lagged_features(processed_data, LAG_FEATURES, LAG_PERIODS)\n",
    "    \n",
    "    print(f\"✓ Lagged features created\")\n",
    "    print(f\"  - Features: {LAG_FEATURES}\")\n",
    "    print(f\"  - Lag periods: 1 to {LAG_PERIODS}\")\n",
    "    print(f\"  - Total lagged features: {len(LAG_FEATURES) * LAG_PERIODS}\")\n",
    "    \n",
    "    # Check for any issues with lagged features\n",
    "    lagged_cols = [f'{feature}_lag_{i}' for feature in LAG_FEATURES for i in range(1, LAG_PERIODS + 1)]\n",
    "    existing_lagged = [col for col in lagged_cols if col in processed_data.columns]\n",
    "    print(f\"  - Successfully created: {len(existing_lagged)} lagged features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in lagged features: {e}\")\n",
    "    raise\n",
    "\n",
    "log_memory_usage(\"After lagged features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Final data cleaning and validation\n",
    "print(\"Step 7: Final data cleaning and validation...\")\n",
    "\n",
    "try:\n",
    "    # Replace infinite and NaN values\n",
    "    processed_data = processed_data.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Handle missing values using the utility function\n",
    "    numeric_columns = processed_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    processed_data = impute_missing_values(processed_data, numeric_columns)\n",
    "    \n",
    "    # Final statistics\n",
    "    print(\"\\n=== Final Dataset Statistics ===\")\n",
    "    print(f\"Shape: {processed_data.shape}\")\n",
    "    print(f\"Date range: {processed_data['vote_date'].min()} to {processed_data['vote_date'].max()}\")\n",
    "    print(f\"Number of DAOs: {processed_data['Slug_Santiment'].nunique()}\")\n",
    "    \n",
    "    # Check final missing values\n",
    "    final_missing = processed_data.isnull().sum().sum()\n",
    "    print(f\"Total missing values: {final_missing:,}\")\n",
    "    \n",
    "    # Validate feature columns exist\n",
    "    available_features = [col for col in FEATURE_COLUMNS if col in processed_data.columns]\n",
    "    missing_features = [col for col in FEATURE_COLUMNS if col not in processed_data.columns]\n",
    "    \n",
    "    print(f\"Available configured features: {len(available_features)}/{len(FEATURE_COLUMNS)}\")\n",
    "    if missing_features:\n",
    "        print(f\"Missing configured features: {missing_features}\")\n",
    "    \n",
    "    # Save processed data\n",
    "    output_path = os.path.join(PROCESSED_DATA_PATH, 'processed_dao_data.csv')\n",
    "    processed_data.to_csv(output_path, index=False)\n",
    "    print(f\"✓ Processed data saved to: {output_path}\")\n",
    "    \n",
    "    # Save data quality report\n",
    "    quality_report = {\n",
    "        'total_rows': len(processed_data),\n",
    "        'total_columns': processed_data.shape[1],\n",
    "        'date_range': f\"{processed_data['vote_date'].min()} to {processed_data['vote_date'].max()}\",\n",
    "        'unique_daos': processed_data['Slug_Santiment'].nunique(),\n",
    "        'missing_values': final_missing,\n",
    "        'available_features': len(available_features),\n",
    "        'missing_features': missing_features\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(os.path.join(PROCESSED_DATA_PATH, 'data_quality_report.json'), 'w') as f:\n",
    "        json.dump(quality_report, f, indent=2, default=str)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in final cleaning: {e}\")\n",
    "    raise\n",
    "\n",
    "log_memory_usage(\"After final cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization and validation\n",
    "print(\"Creating data visualizations...\")\n",
    "\n",
    "try:\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Price trend distribution\n",
    "    trend_counts = processed_data['price_trend'].value_counts()\n",
    "    axes[0, 0].bar(['Fall', 'Rise'], [trend_counts.get(0, 0), trend_counts.get(1, 0)], \n",
    "                   color=['red', 'green'], alpha=0.7)\n",
    "    axes[0, 0].set_title('Price Trend Distribution')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = sum(trend_counts.values())\n",
    "    for i, v in enumerate([trend_counts.get(0, 0), trend_counts.get(1, 0)]):\n",
    "        axes[0, 0].text(i, v + total*0.01, f'{v:,}\\n({v/total:.1%})', \n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Market cap distribution (log scale)\n",
    "    market_cap_clean = processed_data['marketcap_usd_cleaned'].dropna()\n",
    "    market_cap_clean = market_cap_clean[market_cap_clean > 0]\n",
    "    if len(market_cap_clean) > 0:\n",
    "        axes[0, 1].hist(np.log10(market_cap_clean), bins=50, alpha=0.7, color='blue')\n",
    "        axes[0, 1].set_title('Market Cap Distribution (Log10)')\n",
    "        axes[0, 1].set_xlabel('Log10(Market Cap in Billions)')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. Governance activity over time\n",
    "    if 'total_votes' in processed_data.columns:\n",
    "        processed_data['vote_date_parsed'] = pd.to_datetime(processed_data['vote_date'])\n",
    "        monthly_gov = processed_data.groupby(\n",
    "            processed_data['vote_date_parsed'].dt.to_period('M')\n",
    "        )['total_votes'].sum()\n",
    "        \n",
    "        if len(monthly_gov) > 0:\n",
    "            axes[1, 0].plot(range(len(monthly_gov)), monthly_gov.values, color='purple')\n",
    "            axes[1, 0].set_title('Monthly Governance Activity')\n",
    "            axes[1, 0].set_xlabel('Month (Index)')\n",
    "            axes[1, 0].set_ylabel('Total Votes')\n",
    "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Correlation heatmap of key features\n",
    "    key_features = ['marketcap_usd_cleaned', 'total_votes', 'unique_sv_total_1h', \n",
    "                    'transaction_volume', 'price_trend']\n",
    "    available_key_features = [f for f in key_features if f in processed_data.columns]\n",
    "    \n",
    "    if len(available_key_features) >= 2:\n",
    "        corr_data = processed_data[available_key_features].corr()\n",
    "        im = axes[1, 1].imshow(corr_data, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        axes[1, 1].set_xticks(range(len(available_key_features)))\n",
    "        axes[1, 1].set_yticks(range(len(available_key_features)))\n",
    "        axes[1, 1].set_xticklabels(available_key_features, rotation=45, ha='right')\n",
    "        axes[1, 1].set_yticklabels(available_key_features)\n",
    "        axes[1, 1].set_title('Feature Correlation Matrix')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=axes[1, 1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plots\n",
    "    plot_path = os.path.join(RESULTS_PATH, 'data_preprocessing_plots.png')\n",
    "    os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Plots saved to: {plot_path}\")\n",
    "    print(\"✓ Data preprocessing completed successfully!\")\n",
    "    print(f\"✓ Dataset ready for model training with {processed_data.shape[1]} features\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n=== Processing Summary ===\")\n",
    "    print(f\"✓ Processed {len(processed_data):,} records\")\n",
    "    print(f\"✓ Created {processed_data.shape[1]} features\")\n",
    "    print(f\"✓ Target variable: {processed_data['price_trend'].notna().sum():,} valid labels\")\n",
    "    print(f\"✓ Data saved to: {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Warning: Error in visualization: {e}\")\n",
    "    print(\"Data processing completed but visualization failed\")\n",
    "\n",
    "log_memory_usage(\"Final memory usage\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
